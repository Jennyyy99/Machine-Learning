{"cells":[{"cell_type":"code","execution_count":1,"id":"cf5a774a","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","import pyspark.sql.functions as f\n","spark = SparkSession.builder.appName('hello').getOrCreate()"]},{"cell_type":"code","execution_count":2,"id":"80f87e83","metadata":{},"outputs":[],"source":["path=f\"gs://{'ornate-producer-329407'}/pyspark_nlp/CONSUMER_COMPLAINTS_TRAIN.csv\"\n","path_class=f\"gs://{'ornate-producer-329407'}/pyspark_nlp/labels.csv\""]},{"cell_type":"code","execution_count":3,"id":"5bb183d7","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df1=spark.read.csv(path, header=True)\n","df2=spark.read.csv(path_class, header=True)"]},{"cell_type":"code","execution_count":4,"id":"5e1e37cb","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import monotonically_increasing_id\n","\n","df_1 = df1.withColumn(\"row_id\", monotonically_increasing_id())\n","df_2 = df2.withColumn(\"row_id\", monotonically_increasing_id())\n","df = df_1.join(df_2, (\"row_id\")).drop(\"row_id\")"]},{"cell_type":"code","execution_count":5,"id":"2228a639","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 3:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","|                text|               class|\n","+--------------------+--------------------+\n","|Duplicate payment...|Bank account or s...|\n","|I bought a money ...|Other financial s...|\n","|\"I filed a proper...|    Credit reporting|\n","|My dispute result...|            Mortgage|\n","|As there is no su...|         Credit card|\n","|TransUnion failed...|Bank account or s...|\n","|TransUnion respon...|    Credit reporting|\n","|I have checked wi...|        Student loan|\n","|It is clear that ...|            Mortgage|\n","|         Please note|         Credit card|\n","|I have also inclu...|    Credit reporting|\n","|This list shows a...|    Credit reporting|\n","|My ex-wife and I ...|        Student loan|\n","|I 've had a credi...|     Debt collection|\n","|This is my fifth ...|            Mortgage|\n","|In an attempt to ...|Checking or savin...|\n","|My student loan i...|     Debt collection|\n","|On XX/XX/XXXX, I ...|         Payday loan|\n","|       On XX/XX/XXXX|    Credit reporting|\n","|I have not receiv...|Bank account or s...|\n","+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df.show()"]},{"cell_type":"code","execution_count":6,"id":"d9e3839d","metadata":{},"outputs":[],"source":["from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n"]},{"cell_type":"code","execution_count":7,"id":"ad906ad0","metadata":{},"outputs":[],"source":["tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n","stop_word_remover=StopWordsRemover(inputCol=\"token_text\", outputCol=\"stop_tokens\")\n","count_vec=CountVectorizer(inputCol=\"stop_tokens\",outputCol=\"c_vec\")\n","idf=IDF(inputCol=\"c_vec\",outputCol=\"tf-idf\")\n","class_to_num=StringIndexer(inputCol=\"class\",outputCol=\"label\")"]},{"cell_type":"code","execution_count":8,"id":"dca2d52d","metadata":{},"outputs":[],"source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.linalg import Vector"]},{"cell_type":"code","execution_count":9,"id":"6505d35f","metadata":{},"outputs":[],"source":["cleaned= VectorAssembler(inputCols=['tf-idf'], outputCol='features')"]},{"cell_type":"code","execution_count":10,"id":"1fa4b107","metadata":{},"outputs":[],"source":["from pyspark.ml.classification import NaiveBayes\n","nb=NaiveBayes(labelCol='label',featuresCol='features')"]},{"cell_type":"code","execution_count":11,"id":"498053a5","metadata":{},"outputs":[],"source":["from pyspark.ml import Pipeline"]},{"cell_type":"code","execution_count":12,"id":"a9ec8eec","metadata":{},"outputs":[],"source":["pipeline=Pipeline(stages=[\n","    class_to_num,\n","    tokenizer,\n","    stop_word_remover,\n","    count_vec,\n","    idf,\n","    cleaned,\n","    nb\n","])"]},{"cell_type":"code","execution_count":13,"id":"48f39c3c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["pred=pipeline.fit(df)"]},{"cell_type":"code","execution_count":14,"id":"153de0c6","metadata":{},"outputs":[],"source":["path_test=f\"gs://{'ornate-producer-329407'}/pyspark_nlp/CONSUMER_COMPLAINTS_TEST.csv\"\n","path_test_class=f\"gs://{'ornate-producer-329407'}/pyspark_nlp/labels_test.csv\""]},{"cell_type":"code","execution_count":15,"id":"bcc008a3","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df1t=spark.read.csv(path_test, header=True)\n","df2t=spark.read.csv(path_test_class, header=True)"]},{"cell_type":"code","execution_count":16,"id":"e4ebd97b","metadata":{},"outputs":[],"source":["df_1t = df1t.withColumn(\"row_id\", monotonically_increasing_id())\n","df_2t = df2t.withColumn(\"row_id\", monotonically_increasing_id())\n","df_test = df_1t.join(df_2t, (\"row_id\")).drop(\"row_id\")"]},{"cell_type":"code","execution_count":17,"id":"dd2a6e63","metadata":{},"outputs":[],"source":["res=pred.transform(df_test)"]},{"cell_type":"code","execution_count":19,"id":"7fbcc422","metadata":{},"outputs":[],"source":["res=res.select('label','prediction')"]},{"cell_type":"code","execution_count":null,"id":"c21a8587","metadata":{},"outputs":[],"source":["res.write.csv(\"gs://ornate-producer-329407/output\")"]},{"cell_type":"code","execution_count":null,"id":"4366b1eb","metadata":{},"outputs":[],"source":["pred.write().overwrite().save(\"gs://ornate-producer-329407/NBmodel\")"]},{"cell_type":"code","execution_count":25,"id":"42e40463","metadata":{},"outputs":[],"source":["from pyspark.ml.evaluation  import MulticlassClassificationEvaluator"]},{"cell_type":"code","execution_count":26,"id":"85881c36","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/11/24 19:31:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1711.5 KiB\n","                                                                                \r"]}],"source":["eval=MulticlassClassificationEvaluator()\n","acc=eval.evaluate(res)"]},{"cell_type":"code","execution_count":27,"id":"53e0fcf9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 77.63243874123462\n"]}],"source":["print(f\"Accuracy: {acc * 100}\")"]},{"cell_type":"code","execution_count":null,"id":"40b08f16","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}