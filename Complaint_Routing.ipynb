{"cells":[{"cell_type":"code","execution_count":1,"id":"cf5a774a","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","import pyspark.sql.functions as f\n","spark = SparkSession.builder.appName('hello').getOrCreate()"]},{"cell_type":"code","execution_count":2,"id":"80f87e83","metadata":{},"outputs":[],"source":["path=f\"gs://{'ornate-producer-329407'}/pyspark_nlp/CONSUMER_COMPLAINTS_TRAIN.csv\"\n","path_class=f\"gs://{'ornate-producer-329407'}/pyspark_nlp/labels.csv\""]},{"cell_type":"code","execution_count":3,"id":"5bb183d7","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df1=spark.read.csv(path, header=True)\n","df2=spark.read.csv(path_class, header=True)"]},{"cell_type":"code","execution_count":4,"id":"5e1e37cb","metadata":{},"outputs":[],"source":["from pyspark.sql.functions import monotonically_increasing_id\n","\n","df_1 = df1.withColumn(\"row_id\", monotonically_increasing_id())\n","df_2 = df2.withColumn(\"row_id\", monotonically_increasing_id())\n","df = df_1.join(df_2, (\"row_id\")).drop(\"row_id\")"]},{"cell_type":"code","execution_count":5,"id":"2228a639","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 3:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+\n","|                text|               class|\n","+--------------------+--------------------+\n","|Duplicate payment...|Bank account or s...|\n","|I bought a money ...|Other financial s...|\n","|\"I filed a proper...|    Credit reporting|\n","|My dispute result...|            Mortgage|\n","|As there is no su...|         Credit card|\n","|TransUnion failed...|Bank account or s...|\n","|TransUnion respon...|    Credit reporting|\n","|I have checked wi...|        Student loan|\n","|It is clear that ...|            Mortgage|\n","|         Please note|         Credit card|\n","|I have also inclu...|    Credit reporting|\n","|This list shows a...|    Credit reporting|\n","|My ex-wife and I ...|        Student loan|\n","|I 've had a credi...|     Debt collection|\n","|This is my fifth ...|            Mortgage|\n","|In an attempt to ...|Checking or savin...|\n","|My student loan i...|     Debt collection|\n","|On XX/XX/XXXX, I ...|         Payday loan|\n","|       On XX/XX/XXXX|    Credit reporting|\n","|I have not receiv...|Bank account or s...|\n","+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df.show()"]},{"cell_type":"code","execution_count":6,"id":"d9e3839d","metadata":{},"outputs":[],"source":["from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n"]},{"cell_type":"code","execution_count":7,"id":"ad906ad0","metadata":{},"outputs":[],"source":["tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n","stop_word_remover=StopWordsRemover(inputCol=\"token_text\", outputCol=\"stop_tokens\")\n","count_vec=CountVectorizer(inputCol=\"stop_tokens\",outputCol=\"c_vec\")\n","idf=IDF(inputCol=\"c_vec\",outputCol=\"tf-idf\")\n","class_to_num=StringIndexer(inputCol=\"class\",outputCol=\"label\")"]},{"cell_type":"code","execution_count":8,"id":"dca2d52d","metadata":{},"outputs":[],"source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.linalg import Vector"]},{"cell_type":"code","execution_count":9,"id":"6505d35f","metadata":{},"outputs":[],"source":["cleaned= VectorAssembler(inputCols=['tf-idf'], outputCol='features')"]},{"cell_type":"code","execution_count":10,"id":"1fa4b107","metadata":{},"outputs":[],"source":["from pyspark.ml.classification import NaiveBayes\n","nb=NaiveBayes(labelCol='label',featuresCol='features')"]},{"cell_type":"code","execution_count":11,"id":"498053a5","metadata":{},"outputs":[],"source":["from pyspark.ml import Pipeline"]},{"cell_type":"code","execution_count":12,"id":"a9ec8eec","metadata":{},"outputs":[],"source":["pipeline=Pipeline(stages=[\n","    class_to_num,\n","    tokenizer,\n","    stop_word_remover,\n","    count_vec,\n","    idf,\n","    cleaned,\n","    nb\n","])"]},{"cell_type":"code","execution_count":13,"id":"48f39c3c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["pred=pipeline.fit(df)"]},{"cell_type":"code","execution_count":14,"id":"153de0c6","metadata":{},"outputs":[],"source":["path_test=f\"gs://{'ornate-producer-329407'}/pyspark_nlp/CONSUMER_COMPLAINTS_TEST.csv\"\n","path_test_class=f\"gs://{'ornate-producer-329407'}/pyspark_nlp/labels_test.csv\""]},{"cell_type":"code","execution_count":15,"id":"bcc008a3","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df1t=spark.read.csv(path_test, header=True)\n","df2t=spark.read.csv(path_test_class, header=True)"]},{"cell_type":"code","execution_count":16,"id":"e4ebd97b","metadata":{},"outputs":[],"source":["df_1t = df1t.withColumn(\"row_id\", monotonically_increasing_id())\n","df_2t = df2t.withColumn(\"row_id\", monotonically_increasing_id())\n","df_test = df_1t.join(df_2t, (\"row_id\")).drop(\"row_id\")"]},{"cell_type":"code","execution_count":17,"id":"dd2a6e63","metadata":{},"outputs":[],"source":["res=pred.transform(df_test)"]},{"cell_type":"code","execution_count":18,"id":"58884672","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/11/24 18:53:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 1295.4 KiB\n","[Stage 23:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n","|                text|               class|label|          token_text|         stop_tokens|               c_vec|              tf-idf|            features|       rawPrediction|         probability|prediction|\n","+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n","|I purchase per pa...|        Prepaid card|  9.0|[i, purchase, per...|[purchase, per, p...|(8000,[18,19,25,2...|(8000,[18,19,25,2...|(8000,[18,19,25,2...|[-1173.9499553877...|[1.29368843478498...|       1.0|\n","|I paid my credit ...|         Credit card|  3.0|[i, paid, my, cre...|[paid, credit, ca...|(8000,[1,19,23,36...|(8000,[1,19,23,36...|(8000,[1,19,23,36...|[-1027.5557474211...|[0.99999999999999...|       0.0|\n","|Our Landlord, XXX...|     Debt collection|  0.0|[our, landlord,, ...|[landlord,, xxxx,...|(8000,[0,1,7,8,12...|(8000,[0,1,7,8,12...|(8000,[0,1,7,8,12...|[-2574.5395971752...|[1.34341192198564...|       3.0|\n","|I was reviewing m...|     Debt collection|  0.0|[i, was, reviewin...|[reviewing, credi...|(8000,[1,2,12,13,...|(8000,[1,2,12,13,...|(8000,[1,2,12,13,...|[-606.43403702048...|[7.84200429462606...|       4.0|\n","|This is my second...|            Mortgage|  2.0|[this, is, my, se...|[second, letter, ...|(8000,[0,2,3,5,9,...|(8000,[0,2,3,5,9,...|(8000,[0,2,3,5,9,...|[-1536.0627111793...|[2.36677725713048...|       2.0|\n","|2.Why did BOA nev...|    Credit reporting|  1.0|[2.why, did, boa,...|[2.why, boa, neve...|(8000,[0,3,14,28,...|(8000,[0,3,14,28,...|(8000,[0,3,14,28,...|[-247.28645970744...|[1.12319011829348...|       2.0|\n","|3. Why did BOA ch...|        Student loan|  5.0|[3., why, did, bo...|[3., boa, charge,...|(8000,[2,22,71,13...|(8000,[2,22,71,13...|(8000,[2,22,71,13...|[-361.81908100242...|[8.47424282862185...|       2.0|\n","|5. On what dates ...|         Credit card|  3.0|[5., on, what, da...|[5., dates, boa, ...|(8000,[0,171,341,...|(8000,[0,171,341,...|(8000,[0,171,341,...|[-244.51705432989...|[3.84687509876030...|       3.0|\n","|6. Did BOA receiv...|     Debt collection|  0.0|[6., did, boa, re...|[6., boa, receive...|(8000,[24,153,201...|(8000,[24,153,201...|(8000,[24,153,201...|[-144.96762709396...|[0.02462705682566...|       3.0|\n","|7. For what reaso...|Bank account or s...|  4.0|[7., for, what, r...|[7., reason, boa,...|(8000,[84,146,341...|(8000,[84,146,341...|(8000,[84,146,341...|[-184.53780557187...|[3.39695383118081...|       5.0|\n","|8. Why was the co...|            Mortgage|  2.0|[8., why, was, th...|[8., correct, mor...|(8000,[3,4,22,26,...|(8000,[3,4,22,26,...|(8000,[3,4,22,26,...|[-847.76719831892...|[1.50977539960499...|       6.0|\n","|10. What exactly ...|Bank account or s...|  4.0|[10., what, exact...|[10., exactly, bo...|(8000,[64,107,148...|(8000,[64,107,148...|(8000,[64,107,148...|[-219.92788652126...|[0.00223118241919...|       9.0|\n","|Please be advised...|            Mortgage|  2.0|[please, be, advi...|[please, advised,...|(8000,[54,64,68,1...|(8000,[54,64,68,1...|(8000,[54,64,68,1...|[-381.12342184458...|[3.99903823815740...|       1.0|\n","|On a few separate...|     Debt collection|  0.0|[on, a, few, sepa...|[separate, occasi...|(8000,[0,1,7,10,1...|(8000,[0,1,7,10,1...|(8000,[0,1,7,10,1...|[-960.55639573620...|[9.20388584760355...|       1.0|\n","|\"I made an \"\" ext...|    Credit reporting|  1.0|[\"i, made, an, \"\"...|[\"i, made, \"\", ex...|(8000,[10,26,34,3...|(8000,[10,26,34,3...|(8000,[10,26,34,3...|[-765.83235794952...|[0.99933810259802...|       0.0|\n","|Synchrony Bank is...|            Mortgage|  2.0|[synchrony, bank,...|[synchrony, bank,...|(8000,[3,5,9,19,1...|(8000,[3,5,9,19,1...|(8000,[3,5,9,19,1...|[-433.68876072129...|[0.99970919091310...|       0.0|\n","|I opened a Synchr...|            Mortgage|  2.0|[i, opened, a, sy...|[opened, synchron...|(8000,[0,2,19,124...|(8000,[0,2,19,124...|(8000,[0,2,19,124...|[-358.99967257098...|[0.99799780922792...|       0.0|\n","|I attempted to pu...|    Credit reporting|  1.0|[i, attempted, to...|[attempted, purch...|(8000,[25,54,77,1...|(8000,[25,54,77,1...|(8000,[25,54,77,1...|[-229.59412422805...|[0.00209539251013...|       2.0|\n","|I then purchased ...|            Mortgage|  2.0|[i, then, purchas...|[purchased, merch...|(8000,[332,368,32...|(8000,[332,368,32...|(8000,[332,368,32...|[-190.50864485295...|[0.24892485310928...|       1.0|\n","|                 Now|            Mortgage|  2.0|               [now]|                  []|        (8000,[],[])|        (8000,[],[])|        (8000,[],[])|[-1.3969695017422...|[0.24734540911930...|       0.0|\n","+--------------------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n","only showing top 20 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["res.show()"]},{"cell_type":"code","execution_count":19,"id":"7fbcc422","metadata":{},"outputs":[],"source":["res=res.select('label','prediction')"]},{"cell_type":"code","execution_count":20,"id":"b5574e40","metadata":{"scrolled":true},"outputs":[{"ename":"AnalysisException","evalue":"path gs://ornate-producer-329407/output already exists.","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)","Cell \u001B[0;32mIn [20], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mres\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgs://ornate-producer-329407/output\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:1372\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[1;32m   1364\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[1;32m   1365\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(compression\u001B[38;5;241m=\u001B[39mcompression, sep\u001B[38;5;241m=\u001B[39msep, quote\u001B[38;5;241m=\u001B[39mquote, escape\u001B[38;5;241m=\u001B[39mescape, header\u001B[38;5;241m=\u001B[39mheader,\n\u001B[1;32m   1366\u001B[0m                nullValue\u001B[38;5;241m=\u001B[39mnullValue, escapeQuotes\u001B[38;5;241m=\u001B[39mescapeQuotes, quoteAll\u001B[38;5;241m=\u001B[39mquoteAll,\n\u001B[1;32m   1367\u001B[0m                dateFormat\u001B[38;5;241m=\u001B[39mdateFormat, timestampFormat\u001B[38;5;241m=\u001B[39mtimestampFormat,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1370\u001B[0m                charToEscapeQuoteEscaping\u001B[38;5;241m=\u001B[39mcharToEscapeQuoteEscaping,\n\u001B[1;32m   1371\u001B[0m                encoding\u001B[38;5;241m=\u001B[39mencoding, emptyValue\u001B[38;5;241m=\u001B[39memptyValue, lineSep\u001B[38;5;241m=\u001B[39mlineSep)\n\u001B[0;32m-> 1372\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n","File \u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1298\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1299\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1300\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1301\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1303\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1304\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1305\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1307\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1308\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n","File \u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:117\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    113\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n","\u001B[0;31mAnalysisException\u001B[0m: path gs://ornate-producer-329407/output already exists."]}],"source":["res.write.csv(\"gs://ornate-producer-329407/output\")"]},{"cell_type":"code","execution_count":null,"id":"4366b1eb","metadata":{},"outputs":[],"source":["pred.write().overwrite().save(\"gs://ornate-producer-329407/NBmodel\")"]},{"cell_type":"code","execution_count":null,"id":"5900b6c8","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}